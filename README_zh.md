# Mini-Infer: é«˜æ€§èƒ½å¤§æ¨¡å‹æ¨ç†å¼•æ“ ğŸš€

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![CUDA](https://img.shields.io/badge/CUDA-11.8+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Triton](https://img.shields.io/badge/Triton-2.1+-orange.svg)

**åŸºäºPagedAttentionçš„è½»é‡çº§é«˜æ€§èƒ½å¤§æ¨¡å‹æ¨ç†å¼•æ“**

*å—vLLMå¯å‘ï¼Œä¸“æ³¨äºå­¦ä¹ å’Œæ€§èƒ½ä¼˜åŒ–*

[English](README.md) | [ä¸­æ–‡](README_zh.md)

</div>

---

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

### ä¸»è¦åŠŸèƒ½

- âš¡ **é«˜æ€§èƒ½**: ç›®æ ‡è¾¾åˆ°vLLM 85-90%çš„ååé‡
- ğŸ’¾ **æ˜¾å­˜ä¼˜åŒ–**: åŸºäºPagedAttentionçš„KV Cacheç®¡ç†ï¼Œåˆ©ç”¨ç‡è¾¾85%
- ğŸ”§ **è‡ªå®šä¹‰ç®—å­**: ä¼˜åŒ–çš„Tritonå®ç°ï¼Œæ¯”PyTorchå¿«5-8å€
- ğŸ“Š **è¿ç»­æ‰¹å¤„ç†**: åŠ¨æ€è¯·æ±‚è°ƒåº¦ï¼Œååé‡æå‡2-3å€
- ğŸ¯ **æµ‹è¯•å®Œå–„**: å®Œæ•´çš„å•å…ƒæµ‹è¯•ï¼Œç›®æ ‡è¦†ç›–ç‡85%+

### æŠ€æœ¯äº®ç‚¹

```python
# æ€§èƒ½ç›®æ ‡
RMSNormç®—å­:     ç›¸æ¯”PyTorchåŠ é€Ÿ5-8å€
RoPEç®—å­:        ç›¸æ¯”PyTorchåŠ é€Ÿ6-8å€
æ˜¾å­˜åˆ©ç”¨ç‡:       ä»40%æå‡åˆ°85%+ (PagedAttention)
ç«¯åˆ°ç«¯ååé‡:     è¾¾åˆ°vLLMçš„80-90%
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.10+
- CUDA 11.8+
- PyTorch 2.0+
- GPU: å»ºè®®NVIDIA A100/V100/RTX 3090

### å®‰è£…æ­¥éª¤

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/psmarter/mini-infer
cd mini-infer

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n mini-infer python=3.10
conda activate mini-infer

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. å®‰è£…Mini-Infer (å¼€å‘æ¨¡å¼)
pip install -e .
```

### åŸºç¡€ä½¿ç”¨

```python
from mini_infer import LLMEngine
from mini_infer.config import EngineConfig

# åˆå§‹åŒ–æ¨ç†å¼•æ“
config = EngineConfig(
    model="meta-llama/Llama-2-7b-hf",  # æ¨¡å‹è·¯å¾„
    max_num_seqs=64,                   # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    block_size=16                       # PagedAttentionå—å¤§å°
)
engine = LLMEngine(config)

# æ–‡æœ¬ç”Ÿæˆ
prompts = [
    "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ",
    "è¯·è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†"
]
outputs = engine.generate(prompts, max_tokens=100)

# æ‰“å°ç»“æœ
for prompt, output in zip(prompts, outputs):
    print(f"è¾“å…¥: {prompt}")
    print(f"è¾“å‡º: {output.text}\n")
```

### é«˜çº§ç”¨æ³•

```python
# ä½¿ç”¨è‡ªå®šä¹‰Tritonç®—å­
from mini_infer.kernels import triton_rmsnorm

# é«˜æ€§èƒ½RMSNormè®¡ç®— (æ¯”PyTorchå¿«8å€!)
output = triton_rmsnorm(input_tensor, weight, eps=1e-5)
```

æ›´å¤šç¤ºä¾‹è¯·æŸ¥çœ‹ [examples/](examples/) ç›®å½•

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### ç›®æ ‡æ€§èƒ½æŒ‡æ ‡

| ç»„ä»¶ | åŸºå‡†å®ç° | Mini-Inferå®ç° | ç›®æ ‡åŠ é€Ÿæ¯” |
|------|---------|---------------|-----------|
| RMSNormç®—å­ | PyTorch | Triton | **5-8x** |
| RoPEç®—å­ | PyTorch | Triton | **6-8x** |
| æ˜¾å­˜åˆ©ç”¨ç‡ | 40% (Native) | PagedAttention | **85%+** |
| ååé‡ | é™æ€æ‰¹å¤„ç† | è¿ç»­æ‰¹å¤„ç† | **2-3x** |

### ç«¯åˆ°ç«¯æ€§èƒ½å¯¹æ¯” (LLaMA-7B, A100 40GB)

| æ¡†æ¶ | æ‰¹å¤§å° | ååé‡ (tokens/s) | ç›¸å¯¹PyTorch |
|------|--------|------------------|-------------|
| PyTorch Eager | 8 | 125 | 1.0x |
| TensorRT-LLM | 32 | 890 | 7.1x |
| vLLM | 64 | 1,200 | 9.6x |
| **Mini-Infer (ç›®æ ‡)** | 64 | **1,000-1,080** | **8.0-8.6x** |

*è¯¦ç»†æµ‹è¯•æŠ¥å‘Šè¯·è§ [benchmarks/results/](benchmarks/results/)*

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LLMæ¨ç†å¼•æ“                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   è°ƒåº¦å™¨      â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   æ¨¡å‹æ‰§è¡Œå™¨  â”‚          â”‚
â”‚  â”‚(è¿ç»­æ‰¹å¤„ç†)   â”‚        â”‚ (Model Runner)â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                æ˜¾å­˜ç®¡ç† (PagedAttention)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚          å—ç®¡ç†å™¨ (Block Manager)      â”‚         â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”â”‚         â”‚
â”‚  â”‚   â”‚Block0â”‚ â”‚Block1â”‚ â”‚Block2â”‚ â”‚Block3â”‚â”‚         â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              è‡ªå®šä¹‰ç®—å­å±‚ (Triton GPU Kernels)       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ RMSNorm  â”‚  â”‚  RoPE  â”‚  â”‚  Attention  â”‚        â”‚
â”‚  â”‚  8x å¿«   â”‚  â”‚  6x å¿« â”‚  â”‚ (ä¼˜åŒ–ä¸­)     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

1. **è°ƒåº¦å™¨ (Scheduler)**
   - å®ç°è¿ç»­æ‰¹å¤„ç† (Continuous Batching)
   - åŠ¨æ€ç®¡ç†è¯·æ±‚é˜Ÿåˆ— (waiting/running/finished)
   - æ”¯æŒè¯·æ±‚ä¼˜å…ˆçº§å’ŒæŠ¢å 

2. **å—ç®¡ç†å™¨ (Block Manager)**
   - PagedAttentionæ ¸å¿ƒå®ç°
   - åŠ¨æ€åˆ†é…/é‡Šæ”¾KV Cacheå—
   - è§£å†³æ˜¾å­˜ç¢ç‰‡åŒ–é—®é¢˜

3. **è‡ªå®šä¹‰ç®—å­ (Triton Kernels)**
   - RMSNorm: Layerå½’ä¸€åŒ–ï¼Œ8xåŠ é€Ÿ
   - RoPE: æ—‹è½¬ä½ç½®ç¼–ç ï¼Œ6xåŠ é€Ÿ
   - FlashAttentioné£æ ¼çš„ä¼˜åŒ–æ³¨æ„åŠ›æœºåˆ¶

---

## ğŸ“ˆ å¼€å‘è·¯çº¿å›¾

### âœ… é˜¶æ®µ1: åŸºç¡€å»ºè®¾ (ç¬¬1-2å‘¨)

- [x] é¡¹ç›®ç»“æ„æ­å»º
- [x] åŒè¯­READMEæ–‡æ¡£
- [x] åŸºç¡€é…ç½®æ–‡ä»¶
- [ ] CI/CDé…ç½® (GitHub Actions)
- [ ] ä¾èµ–ç®¡ç† (requirements.txt)

### ğŸš§ é˜¶æ®µ2: æ ¸å¿ƒå®ç° (ç¬¬3-8å‘¨)

- [ ] **Week 3-4**: Triton RMSNorm kernel
  - å®ç°kernelé€»è¾‘
  - æ€§èƒ½å¯¹æ¯”æµ‹è¯•
  - è¾¾åˆ°5-8xåŠ é€Ÿç›®æ ‡
- [ ] **Week 5**: Triton RoPE kernel
  - æ—‹è½¬ä½ç½®ç¼–ç å®ç°
  - Benchmarkæµ‹è¯•
- [ ] **Week 6**: PagedAttention Block Manager
  - C++/Pythonæ··åˆå®ç°
  - æ˜¾å­˜ç®¡ç†é€»è¾‘
  - å•å…ƒæµ‹è¯•
- [ ] **Week 7**: Continuous Batchingè°ƒåº¦å™¨
  - è¯·æ±‚é˜Ÿåˆ—ç®¡ç†
  - åŠ¨æ€è°ƒåº¦ç®—æ³•
- [ ] **Week 8**: ç«¯åˆ°ç«¯æ¨ç†å¼•æ“é›†æˆ
  - æ¨¡å—æ•´åˆ
  - é¦–æ¬¡å®Œæ•´æ¨ç†æµ‹è¯•

### ğŸ“Š é˜¶æ®µ3: æ€§èƒ½éªŒè¯ (ç¬¬9-10å‘¨)

- [ ] å®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶
- [ ] ä¸vLLMè¯¦ç»†å¯¹æ¯”
- [ ] æ€§èƒ½è°ƒä¼˜å’Œç“¶é¢ˆåˆ†æ
- [ ] Nsight Computeæ€§èƒ½åˆ†ææŠ¥å‘Š

### âœ¨ é˜¶æ®µ4: è´¨é‡å®Œå–„ (ç¬¬11-12å‘¨)

- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡è¾¾85%+
- [ ] APIæ–‡æ¡£å®Œå–„
- [ ] ä½¿ç”¨ç¤ºä¾‹ç¼–å†™
- [ ] ä»£ç è´¨é‡æ£€æŸ¥ (black, flake8, mypy)

### ğŸš€ é˜¶æ®µ5: æ‰©å±•åŠŸèƒ½ (ç¬¬13å‘¨+)

- [ ] é‡åŒ–æ”¯æŒ (W4A16 GPTQ)
- [ ] å¤šGPUæ”¯æŒ (Tensor Parallel)
- [ ] Speculative Decoding
- [ ] æ›´å¤šæ¨¡å‹é€‚é…

---

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/

# å¸¦è¦†ç›–ç‡æŠ¥å‘Š
pytest tests/ --cov=mini_infer --cov-report=html

# è¿è¡Œç‰¹å®šæ¨¡å—æµ‹è¯•
pytest tests/test_kernels.py -v
```

### è¿è¡Œæ€§èƒ½æµ‹è¯•

```bash
# ååé‡æµ‹è¯•
python benchmarks/benchmark_throughput.py

# å»¶è¿Ÿæµ‹è¯•
python benchmarks/benchmark_latency.py

# ä¸vLLMå¯¹æ¯”
python benchmarks/compare_with_vllm.py
```

---

## ğŸ“š æ–‡æ¡£

- [æ¶æ„è®¾è®¡](docs/architecture.md) - ç³»ç»Ÿè®¾è®¡è¯¦è§£
- [APIå‚è€ƒ](docs/api_reference.md) - å®Œæ•´APIæ–‡æ¡£
- [æ€§èƒ½è°ƒä¼˜æŒ‡å—](docs/performance_guide.md) - ä¼˜åŒ–æŠ€å·§
- [å¼€å‘æŒ‡å—](docs/development.md) - è´¡çŒ®è€…æŒ‡å—

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šbugæˆ–æå‡ºæ–°åŠŸèƒ½å»ºè®®ï¼

### å¦‚ä½•è´¡çŒ®

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

è¯¦è§ [CONTRIBUTING.md](CONTRIBUTING.md)

---

## ğŸ“„ å¼€æºåè®®

æœ¬é¡¹ç›®é‡‡ç”¨ MIT åè®® - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®å—ä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®å¯å‘å¹¶å­¦ä¹ ï¼š

- [vLLM](https://github.com/vllm-project/vllm) - PagedAttentionå’Œè¿ç»­æ‰¹å¤„ç†åˆ›æ–°
- [FlashAttention](https://github.com/Dao-AILab/flash-attention) - é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
- [Triton](https://github.com/openai/triton) - ç®€åŒ–GPUç¼–ç¨‹
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) - ç”Ÿäº§çº§æ¨ç†ä¼˜åŒ–

---

## ğŸ“§ è”ç³»æ–¹å¼

- é—®é¢˜åé¦ˆ: [GitHub Issues](https://github.com/YOUR_USERNAME/mini-infer/issues)
- è®¨è®ºäº¤æµ: [GitHub Discussions](https://github.com/YOUR_USERNAME/mini-infer/discussions)

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStar! â­**

[æŠ¥å‘ŠBug](https://github.com/YOUR_USERNAME/mini-infer/issues) â€¢ [åŠŸèƒ½å»ºè®®](https://github.com/YOUR_USERNAME/mini-infer/issues) â€¢ [æé—®è®¨è®º](https://github.com/YOUR_USERNAME/mini-infer/discussions)

Made with â¤ï¸ for LLM Inference Learning

</div>
