# Examples | ç¤ºä¾‹

This directory contains usage examples for Mini-Infer.

æ­¤ç›®å½•åŒ…å«Mini-Inferçš„ä½¿ç”¨ç¤ºä¾‹ã€‚

---

## Available Examples | å¯ç”¨ç¤ºä¾‹

### [quickstart.py](file:///h:/å°±ä¸š/mini-infer/examples/quickstart.py)

Basic usage example showing how to initialize the engine and generate text.

å±•ç¤ºå¦‚ä½•åˆå§‹åŒ–å¼•æ“å’Œç”Ÿæˆæ–‡æœ¬çš„åŸºç¡€ç”¨æ³•ç¤ºä¾‹ã€‚

**Status | çŠ¶æ€:** ğŸš§ Preview - Implementation in progress

---

## Running Examples | è¿è¡Œç¤ºä¾‹

> [!NOTE]
> Examples are currently **preview versions** showing the intended API.
> Full implementation coming soon as core features are completed.
>
> ç¤ºä¾‹ç›®å‰æ˜¯**é¢„è§ˆç‰ˆæœ¬**ï¼Œå±•ç¤ºé¢„æœŸçš„APIã€‚
> å®Œæ•´å®ç°å³å°†æ¨å‡ºï¼Œå¾…æ ¸å¿ƒåŠŸèƒ½å®Œæˆåã€‚

### Prerequisites | å‰ç½®æ¡ä»¶

1. Install Mini-Infer:

   ```bash
   pip install -e .
   ```

2. Ensure CUDA is available:

   ```python
   python -c "import torch; print(torch.cuda.is_available())"
   ```

### Run Quickstart | è¿è¡Œå¿«é€Ÿå¼€å§‹

```bash
python examples/quickstart.py
```

---

## Planned Examples | è®¡åˆ’ä¸­çš„ç¤ºä¾‹

### Basic Inference | åŸºç¡€æ¨ç†

```python
# examples/basic_inference.py (coming soon)
from mini_infer import LLMEngine
from mini_infer.config import EngineConfig

config = EngineConfig(model="meta-llama/Llama-2-7b-hf")
engine = LLMEngine(config)

outputs = engine.generate(
    prompts=["Explain machine learning in simple terms."],
    max_tokens=200
)
print(outputs[0].text)
```

### Batch Processing | æ‰¹é‡å¤„ç†

```python
# examples/batch_processing.py (coming soon)
prompts = [
    "Write a haiku about coding",
    "Explain recursion",
    "What is a transformer model?"
]

outputs = engine.generate(
    prompts=prompts,
    max_tokens=100,
    temperature=0.7
)

for i, output in enumerate(outputs):
    print(f"\n=== Prompt {i+1} ===")
    print(output.text)
```

### Streaming Generation | æµå¼ç”Ÿæˆ

```python
# examples/streaming.py (coming soon)
for token in engine.generate_stream(
    prompt="Write a short story",
    max_tokens=500
):
    print(token, end='', flush=True)
```

### Custom Sampling | è‡ªå®šä¹‰é‡‡æ ·

```python
# examples/custom_sampling.py (coming soon)
from mini_infer.sampling import SamplingParams

# Creative sampling
creative_params = SamplingParams(
    temperature=0.9,
    top_p=0.95,
    top_k=50
)

# Deterministic sampling
deterministic_params = SamplingParams(
    temperature=0.0,  # Greedy
)

outputs_creative = engine.generate(
    prompts=["Write a creative story"],
    sampling_params=creative_params
)

outputs_deterministic = engine.generate(
    prompts=["Summarize: ..."],
    sampling_params=deterministic_params
)
```

### Benchmarking | åŸºå‡†æµ‹è¯•

```python
# examples/benchmark.py (coming soon)
import time

prompts = ["Test prompt"] * 32  # Batch of 32

start = time.time()
outputs = engine.generate(prompts, max_tokens=100)
end = time.time()

throughput = (32 * 100) / (end - start)
print(f"Throughput: {throughput:.2f} tokens/sec")
```

---

## Example Output | ç¤ºä¾‹è¾“å‡º

### Expected Output Format | é¢„æœŸè¾“å‡ºæ ¼å¼

```python
# GenerateOutput object
output = outputs[0]

print(f"Generated text: {output.text}")
print(f"Token IDs: {output.token_ids}")
print(f"Finish reason: {output.finish_reason}")  # 'stop' or 'length'
print(f"Log probability: {output.cumulative_logprob}")
```

---

## Tips for Examples | ç¤ºä¾‹æç¤º

### Memory Management | å†…å­˜ç®¡ç†

```python
# For large batches, monitor memory
config = EngineConfig(
    model="meta-llama/Llama-2-7b-hf",
    max_num_seqs=32,  # Reduce if OOM
    gpu_memory_utilization=0.85
)
```

### Model Selection | æ¨¡å‹é€‰æ‹©

```python
# Start with smaller models for testing
models = {
    "small": "meta-llama/Llama-2-7b-hf",  # ~14GB VRAM
    "medium": "meta-llama/Llama-2-13b-hf",  # ~26GB VRAM
    "large": "meta-llama/Llama-2-70b-hf",  # ~140GB VRAM (multi-GPU)
}
```

### Error Handling | é”™è¯¯å¤„ç†

```python
from mini_infer.exceptions import OutOfMemoryError, ModelNotFoundError

try:
    engine = LLMEngine(config)
    outputs = engine.generate(prompts)
except OutOfMemoryError:
    print("Reduce batch size or use smaller model")
except ModelNotFoundError:
    print("Check model name/path")
```

---

## Contributing Examples | è´¡çŒ®ç¤ºä¾‹

Have a useful example? Contribute it!

æœ‰æœ‰ç”¨çš„ç¤ºä¾‹å—ï¼Ÿè´¡çŒ®å®ƒï¼

1. Create a new `.py` file in `examples/`
2. Add clear comments and docstrings
3. Update this README
4. Submit a pull request

See [CONTRIBUTING.md](file:///h:/å°±ä¸š/mini-infer/CONTRIBUTING.md) for guidelines.

---

## Getting Help | è·å–å¸®åŠ©

- [API Reference](file:///h:/å°±ä¸š/mini-infer/docs/api_reference.md) - Detailed API documentation
- [FAQ](file:///h:/å°±ä¸š/mini-infer/docs/faq.md) - Common questions
- [GitHub Issues](https://github.com/psmarter/mini-infer/issues) - Bug reports and questions
