# Mini-Infer: High-Performance LLM Inference Engine ğŸš€

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![CUDA](https://img.shields.io/badge/CUDA-11.8+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)
![Triton](https://img.shields.io/badge/Triton-2.1+-orange.svg)

**A lightweight yet powerful LLM inference engine with PagedAttention**

**åŸºäºPagedAttentionçš„è½»é‡çº§é«˜æ€§èƒ½å¤§æ¨¡å‹æ¨ç†å¼•æ“**

*Inspired by vLLM, optimized for learning and performance*

[English](#english) | [ä¸­æ–‡](#chinese)

</div>

---

<a name="english"></a>

## ğŸŒŸ Features

### Core Capabilities

- âš¡ **High Performance**: Target 85-90% of vLLM throughput
- ğŸ’¾ **Memory Efficient**: PagedAttention-based KV Cache management
- ğŸ”§ **Custom Kernels**: Optimized Triton implementations (5-8x faster than PyTorch)
- ğŸ“Š **Continuous Batching**: Dynamic request scheduling for better throughput
- ğŸ¯ **Well-Tested**: Comprehensive unit tests (target 85%+ coverage)

### Technical Highlights

```python
# Performance Targets
RMSNorm Kernel:      5-8x speedup vs PyTorch
Memory Utilization:  85%+ (PagedAttention)
End-to-End:          80-90% of vLLM performance
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/mini-infer.git
cd mini-infer

# Create virtual environment
conda create -n mini-infer python=3.10
conda activate mini-infer

# Install dependencies (coming soon)
pip install -r requirements.txt
```

### Basic Usage (Preview)

```python
from mini_infer import LLMEngine
from mini_infer.config import EngineConfig

# Initialize engine
config = EngineConfig(
    model="meta-llama/Llama-2-7b-hf",
    max_num_seqs=64,
    block_size=16
)
engine = LLMEngine(config)

# Generate
prompts = ["Hello, how are you?"]
outputs = engine.generate(prompts, max_tokens=100)
print(outputs[0].text)
```

---

## ğŸ“Š Performance Benchmarks (Coming Soon)

### Target Performance

| Component | Baseline | Mini-Infer | Target Speedup |
|-----------|----------|------------|----------------|
| RMSNorm Kernel | PyTorch | Triton | 5-8x |
| RoPE Kernel | PyTorch | Triton | 6-8x |
| Memory Util | 40% | PagedAttention | 85%+ |
| Throughput | Static Batch | Continuous Batch | 2-3x |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         LLM Engine                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚Scheduler â”‚â”€â”€â”€â–¶â”‚Model     â”‚      â”‚
â”‚  â”‚(C-Batch) â”‚    â”‚Runner    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Memory Management               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Block Manager           â”‚      â”‚
â”‚  â”‚  (PagedAttention)        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Custom Kernels (Triton)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚RMSNormâ”‚ â”‚RoPEâ”‚ â”‚Attention â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Roadmap

### Phase 1: Foundation (Weeks 1-2) âœ…

- [x] Project structure
- [x] Basic documentation
- [ ] Development environment setup

### Phase 2: Core Implementation (Weeks 3-8)

- [ ] Triton kernels (RMSNorm, RoPE)
- [ ] PagedAttention Block Manager
- [ ] Continuous Batching Scheduler
- [ ] End-to-end inference engine

### Phase 3: Performance & Testing (Weeks 9-10)

- [ ] Comprehensive benchmarks
- [ ] Unit tests (85%+ coverage)
- [ ] Performance optimization

### Phase 4: Documentation & Polish (Weeks 11-12)

- [ ] API documentation
- [ ] Usage examples
- [ ] Technical blog posts

---

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

This project is inspired by and learns from:

- [vLLM](https://github.com/vllm-project/vllm) - PagedAttention and continuous batching
- [FlashAttention](https://github.com/Dao-AILab/flash-attention) - Efficient attention mechanisms
- [Triton](https://github.com/openai/triton) - GPU programming framework

---

<a name="chinese"></a>

## ğŸŒŸ æ ¸å¿ƒç‰¹æ€§

### ä¸»è¦åŠŸèƒ½

- âš¡ **é«˜æ€§èƒ½**: ç›®æ ‡è¾¾åˆ°vLLM 85-90%çš„ååé‡
- ğŸ’¾ **æ˜¾å­˜ä¼˜åŒ–**: åŸºäºPagedAttentionçš„KV Cacheç®¡ç†
- ğŸ”§ **è‡ªå®šä¹‰ç®—å­**: ä¼˜åŒ–çš„Tritonå®ç° (æ¯”PyTorchå¿«5-8å€)
- ğŸ“Š **è¿ç»­æ‰¹å¤„ç†**: åŠ¨æ€è¯·æ±‚è°ƒåº¦ï¼Œæå‡ååé‡
- ğŸ¯ **æµ‹è¯•å®Œå–„**: å®Œæ•´çš„å•å…ƒæµ‹è¯• (ç›®æ ‡è¦†ç›–ç‡85%+)

### æŠ€æœ¯äº®ç‚¹

```python
# æ€§èƒ½ç›®æ ‡
RMSNormç®—å­:     ç›¸æ¯”PyTorchåŠ é€Ÿ5-8å€
æ˜¾å­˜åˆ©ç”¨ç‡:       85%+ (PagedAttention)
ç«¯åˆ°ç«¯æ€§èƒ½:       vLLMçš„80-90%
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/YOUR_USERNAME/mini-infer.git
cd mini-infer

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n mini-infer python=3.10
conda activate mini-infer

# å®‰è£…ä¾èµ– (å³å°†æ¨å‡º)
pip install -r requirements.txt
```

### åŸºç¡€ä½¿ç”¨ (é¢„è§ˆ)

```python
from mini_infer import LLMEngine
from mini_infer.config import EngineConfig

# åˆå§‹åŒ–å¼•æ“
config = EngineConfig(
    model="meta-llama/Llama-2-7b-hf",
    max_num_seqs=64,
    block_size=16
)
engine = LLMEngine(config)

# ç”Ÿæˆæ–‡æœ¬
prompts = ["ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ"]
outputs = engine.generate(prompts, max_tokens=100)
print(outputs[0].text)
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯• (å³å°†æ¨å‡º)

### ç›®æ ‡æ€§èƒ½

| ç»„ä»¶ | åŸºå‡† | Mini-Infer | ç›®æ ‡åŠ é€Ÿæ¯” |
|------|------|------------|-----------|
| RMSNormç®—å­ | PyTorch | Triton | 5-8x |
| RoPEç®—å­ | PyTorch | Triton | 6-8x |
| æ˜¾å­˜åˆ©ç”¨ç‡ | 40% | PagedAttention | 85%+ |
| ååé‡ | é™æ€æ‰¹å¤„ç† | è¿ç»­æ‰¹å¤„ç† | 2-3x |

---

## ğŸ“ˆ å¼€å‘è·¯çº¿å›¾

### é˜¶æ®µ1: åŸºç¡€å»ºè®¾ (ç¬¬1-2å‘¨) âœ…

- [x] é¡¹ç›®ç»“æ„
- [x] åŸºç¡€æ–‡æ¡£
- [ ] å¼€å‘ç¯å¢ƒé…ç½®

### é˜¶æ®µ2: æ ¸å¿ƒå®ç° (ç¬¬3-8å‘¨)

- [ ] Tritonç®—å­ (RMSNorm, RoPE)
- [ ] PagedAttentionå—ç®¡ç†å™¨
- [ ] è¿ç»­æ‰¹å¤„ç†è°ƒåº¦å™¨
- [ ] ç«¯åˆ°ç«¯æ¨ç†å¼•æ“

### é˜¶æ®µ3: æ€§èƒ½ä¸æµ‹è¯• (ç¬¬9-10å‘¨)

- [ ] å®Œæ•´çš„åŸºå‡†æµ‹è¯•
- [ ] å•å…ƒæµ‹è¯• (è¦†ç›–ç‡85%+)
- [ ] æ€§èƒ½ä¼˜åŒ–

### é˜¶æ®µ4: æ–‡æ¡£ä¸å®Œå–„ (ç¬¬11-12å‘¨)

- [ ] APIæ–‡æ¡£
- [ ] ä½¿ç”¨ç¤ºä¾‹
- [ ] æŠ€æœ¯åšå®¢

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚

---

## ğŸ“„ å¼€æºåè®®

æœ¬é¡¹ç›®é‡‡ç”¨MITåè®® - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®å—ä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®å¯å‘ï¼š

- [vLLM](https://github.com/vllm-project/vllm) - PagedAttentionå’Œè¿ç»­æ‰¹å¤„ç†
- [FlashAttention](https://github.com/Dao-AILab/flash-attention) - é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
- [Triton](https://github.com/openai/triton) - GPUç¼–ç¨‹æ¡†æ¶

---

<div align="center">

**â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStar! â­**

**â­ If you find this project helpful, please star it! â­**

</div>
